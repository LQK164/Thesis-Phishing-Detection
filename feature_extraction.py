import os
import re
import time
import urllib.parse
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from functools import partial
from typing import Any

# from selenium.webdriver.chrome.service import Service
# from selenium.webdriver.common.keys import Keys
# from selenium.webdriver.common.by import By
from urllib.parse import urlparse

import dns.resolver
import Levenshtein
import pandas as pd

# import polars as pl
import requests
import tldextract
import whois
from bs4 import BeautifulSoup

HINTS = [
    "wp",
    "login",
    "includes",
    "admin",
    "content",
    "site",
    "images",
    "js",
    "alibaba",
    "css",
    "myaccount",
    "dropbox",
    "themes",
    "plugins",
    "signin",
    "view",
]

#################################################################################################################################
#               Having IP address in hostname
#################################################################################################################################


def having_ip_address(url: str):
    match = re.search(
        "(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\."
        "([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|"  # IPv4
        "((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)|"  # IPv4 in hexadecimal
        "(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}|"
        "[0-9a-fA-F]{7}",
        url,
    )  # Ipv6
    if match:
        return 1
    else:
        return 0


#################################################################################################################################
#               URL hostname length
#################################################################################################################################


def url_length(url: str):
    return len(url)


#################################################################################################################################
#               URL shortening
#################################################################################################################################


def shortening_service(full_url: str):
    match = re.search(
        r"bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|"
        r"yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|"
        r"short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|"
        r"doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|"
        r"db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|"
        r"q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|"
        r"x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|"
        r"tr\.im|link\.zip\.net",
        full_url,
    )
    if match:
        return 1
    else:
        return 0


#################################################################################################################################
#               Count at (@) symbol at base url
#################################################################################################################################


def count_at(base_url: str):
    return base_url.count("@")


#################################################################################################################################
#               Count comma (,) symbol at base url
#################################################################################################################################


def count_comma(base_url: str):
    return base_url.count(",")


#################################################################################################################################
#               Count dollar ($) symbol at base url
#################################################################################################################################


def count_dollar(base_url: str):
    return base_url.count("$")


#################################################################################################################################
#               Having semicolon (;) symbol at base url
#################################################################################################################################


def count_semicolon(url: str):
    return url.count(";")


#################################################################################################################################
#               Count (space, %20) symbol at base url (Das'19)
#################################################################################################################################


def count_space(base_url: str):
    return base_url.count(" ") + base_url.count("%20")


#################################################################################################################################
#               Count and (&) symbol at base url (Das'19)
#################################################################################################################################


def count_and(base_url: str):
    return base_url.count("&")


#################################################################################################################################
#               Count redirection (//) symbol at full url
#################################################################################################################################


def count_double_slash(full_url: str):
    list = [x.start(0) for x in re.finditer("//", full_url)]
    if list[len(list) - 1] > 6:
        return 1
    else:
        return 0
    # TODO: fix me
    return full_url.count("//")


#################################################################################################################################
#               Count slash (/) symbol at full url
#################################################################################################################################


def count_slash(full_url: str):
    return full_url.count("/")


#################################################################################################################################
#               Count equal (=) symbol at base url
#################################################################################################################################


def count_equal(base_url: str):
    return base_url.count("=")


#################################################################################################################################
#               Count percentage (%) symbol at base url (Chiew2019)
#################################################################################################################################


def count_percentage(base_url: str):
    return base_url.count("%")


#################################################################################################################################
#               Count exclamation (?) symbol at base url
#################################################################################################################################


def count_exclamation(base_url: str):
    return base_url.count("?")


#################################################################################################################################
#               Count underscore (_) symbol at base url
#################################################################################################################################


def count_underscore(base_url: str):
    return base_url.count("_")


#################################################################################################################################
#               Count dash (-) symbol at base url
#################################################################################################################################


def count_hyphens(base_url: str):
    return base_url.count("-")


#################################################################################################################################
#              Count number of dots in hostname
#################################################################################################################################


def count_dots(hostname: str):
    return hostname.count(".")


#################################################################################################################################
#              Count number of colon (:) symbol
#################################################################################################################################


def count_colon(url: str):
    return url.count(":")


#################################################################################################################################
#               Count number of stars (*) symbol (Srinivasa Rao'19)
#################################################################################################################################


def count_star(url: str):
    return url.count("*")


#################################################################################################################################
#               Count number of OR (|) symbol (Srinivasa Rao'19)
#################################################################################################################################


def count_or(url: str):
    return url.count("|")


#################################################################################################################################
#               Path entension != .txt
#################################################################################################################################


def path_extension(url_path: str):
    if url_path.endswith(".txt"):
        return 1
    return 0


#################################################################################################################################
#               Having multiple http or https in url path
#################################################################################################################################


def count_http_token(url_path: str):
    return url_path.count("http")


#################################################################################################################################
#               Uses https protocol
#################################################################################################################################


def https_token(scheme: str):
    if scheme == "https":
        return 0
    return 1


#################################################################################################################################
#               Ratio of digits in hostname
#################################################################################################################################


def ratio_digits(hostname: str):
    return len(re.sub("[^0-9]", "", hostname)) / len(hostname)


#################################################################################################################################
#               Count number of digits in domain/subdomain/path
#################################################################################################################################


def count_digits(line: str):
    return len(re.sub("[^0-9]", "", line))


#################################################################################################################################
#              Checks if tilde symbol exist in webpage URL (Chiew2019)
#################################################################################################################################


def count_tilde(full_url: str):
    if full_url.count("~") > 0:
        return 1
    return 0


#################################################################################################################################
#               number of phish-hints in url path
#################################################################################################################################


def phish_hints(url_path: str):
    count = 0
    for hint in HINTS:
        count += url_path.lower().count(hint)
    return count


#################################################################################################################################
#               Check if TLD exists in the path
#################################################################################################################################


def tld_in_path(tld: str, path: str):
    if path.lower().count(tld) > 0:
        return 1
    return 0


#################################################################################################################################
#               Check if tld is used in the subdomain
#################################################################################################################################


def tld_in_subdomain(tld: str, subdomain: str):
    if subdomain.count(tld) > 0:
        return 1
    return 0


#################################################################################################################################
#               Check if TLD in bad position (Chiew2019)
#################################################################################################################################


def tld_in_bad_position(tld: str, subdomain: str, path: str):
    if tld_in_path(tld, path) == 1 or tld_in_subdomain(tld, subdomain) == 1:
        return 1
    return 0


#################################################################################################################################
#               Abnormal subdomain starting with wwww-, wwNN
#################################################################################################################################


def abnormal_subdomain(url: str):
    if re.search(r"(http[s]?://(w[w]?|\d))([w]?(\d|-))", url):
        return 1
    return 0


#################################################################################################################################
#               Number of redirection
#################################################################################################################################


def count_redirection(page: requests.Response):
    return len(page.history)


#################################################################################################################################
#               Number of redirection to different domains
#################################################################################################################################


def count_external_redirection(page: requests.Response, domain: str):
    count = 0
    if len(page.history) == 0:
        return 0
    else:
        for i, response in enumerate(page.history, 1):
            if domain.lower() not in response.url.lower():
                count += 1
            return count


#################################################################################################################################
#               Consecutive Character Repeat (Sahingoz2019)
#################################################################################################################################


def char_repeat(words_raw: list[str]):
    def __all_same(items: str):
        return all(x == items[0] for x in items)

    repeat = {"2": 0, "3": 0, "4": 0, "5": 0}
    part = [2, 3, 4, 5]

    for word in words_raw:
        for char_repeat_count in part:
            for i in range(len(word) - char_repeat_count + 1):
                sub_word = word[i : i + char_repeat_count]
                if __all_same(sub_word):
                    repeat[str(char_repeat_count)] = repeat[str(char_repeat_count)] + 1
    return sum(list(repeat.values()))


#################################################################################################################################
#               puny code in domain (Sahingoz2019)
#################################################################################################################################


def punycode(url: str):
    if url.startswith("http://xn--") or url.startswith("http://xn--"):
        return 1
    else:
        return 0


#################################################################################################################################
#               domain in brand list (Sahingoz2019)
#################################################################################################################################


def domain_in_brand(domain: str, urls: list[str]):
    if domain in urls:
        return 1
    else:
        return 0


def domain_in_brand1(domain: str, urls: list[str]):
    for d in urls:
        if len(Levenshtein.editops(domain.lower(), d.lower())) < 2:
            return 1
    return 0


#################################################################################################################################
#               brand name in path (Srinivasa-Rao2019)
#################################################################################################################################


def brand_in_path(domain: str, path: str, urls: list[str]):
    for b in urls:
        if "." + b + "." in path and b not in domain:
            return 1
    return 0


#################################################################################################################################
#               count www in url words (Sahingoz2019)
#################################################################################################################################


def check_www(words_raw: list[str]):
    count = 0
    for word in words_raw:
        if not word.find("www") == -1:
            count += 1
    return count


#################################################################################################################################
#               count com in url words (Sahingoz2019)
#################################################################################################################################


def check_com(words_raw: list[str]):
    count = 0
    for word in words_raw:
        if not word.find("com") == -1:
            count += 1
    return count


#################################################################################################################################
#               check port presence in domain
#################################################################################################################################


def port(url: str):
    if re.search(
        r"^[a-z][a-z0-9+\-.]*://([a-z0-9\-._~%!$&'()*+,;=]+@)?([a-z0-9\-._~%]+|\[[a-z0-9\-._~%!$&'()*+,;=:]+\]):([0-9]+)",
        url,
    ):
        return 1
    return 0


#################################################################################################################################
#               length of raw word list (Sahingoz2019)
#################################################################################################################################


def length_word_raw(words_raw):
    return len(words_raw)


#################################################################################################################################
#               count average word length in raw word list (Sahingoz2019)
#################################################################################################################################


def average_word_length(words_raw: list[str]):
    if len(words_raw) == 0:
        return 0
    return sum(len(word) for word in words_raw) / len(words_raw)


#################################################################################################################################
#               longest word length in raw word list (Sahingoz2019)
#################################################################################################################################


def longest_word_length(words_raw: list[str]):
    if len(words_raw) == 0:
        return 0
    return max(len(word) for word in words_raw)


#################################################################################################################################
#               shortest word length in raw word list (Sahingoz2019)
#################################################################################################################################


def shortest_word_length(words_raw: list[str]):
    if len(words_raw) == 0:
        return 0
    return min(len(word) for word in words_raw)


#################################################################################################################################
#               prefix suffix
#################################################################################################################################


def prefix_suffix(url: str):
    if re.findall(r"https?://[^\-]+-[^\-]+/", url):
        return 1
    else:
        return 0


#################################################################################################################################
#               count subdomain
#################################################################################################################################


def count_subdomain(url: str):
    if len(re.findall(r"\.", url)) == 1:
        return 1
    elif len(re.findall(r"\.", url)) == 2:
        return 2
    else:
        return 3


#################################################################################################################################
#               Statistical report
#################################################################################################################################

import socket


def statistical_report(url: str, domain: str):
    url_match = re.search(
        r"at\.ua|usa\.cc|baltazarpresentes\.com\.br|pe\.hu|esy\.es|hol\.es|sweddy\.com|myjino\.ru|96\.lt|ow\.ly",
        url,
    )
    try:
        ip_address = socket.gethostbyname(domain)
        ip_match = re.search(
            r"146\.112\.61\.108|213\.174\.157\.151|121\.50\.168\.88|192\.185\.217\.116|78\.46\.211\.158|181\.174\.165\.13|46\.242\.145\.103|121\.50\.168\.40|83\.125\.22\.219|46\.242\.145\.98|"
            r"107\.151\.148\.44|107\.151\.148\.107|64\.70\.19\.203|199\.184\.144\.27|107\.151\.148\.108|107\.151\.148\.109|119\.28\.52\.61|54\.83\.43\.69|52\.69\.166\.231|216\.58\.192\.225|"
            r"118\.184\.25\.86|67\.208\.74\.71|23\.253\.126\.58|104\.239\.157\.210|175\.126\.123\.219|141\.8\.224\.221|10\.10\.10\.10|43\.229\.108\.32|103\.232\.215\.140|69\.172\.201\.153|"
            r"216\.218\.185\.162|54\.225\.104\.146|103\.243\.24\.98|199\.59\.243\.120|31\.170\.160\.61|213\.19\.128\.77|62\.113\.226\.131|208\.100\.26\.234|195\.16\.127\.102|195\.16\.127\.157|"
            r"34\.196\.13\.28|103\.224\.212\.222|172\.217\.4\.225|54\.72\.9\.51|192\.64\.147\.141|198\.200\.56\.183|23\.253\.164\.103|52\.48\.191\.26|52\.214\.197\.72|87\.98\.255\.18|209\.99\.17\.27|"
            r"216\.38\.62\.18|104\.130\.124\.96|47\.89\.58\.141|78\.46\.211\.158|54\.86\.225\.156|54\.82\.156\.19|37\.157\.192\.102|204\.11\.56\.48|110\.34\.231\.42",
            ip_address,
        )
        if url_match or ip_match:
            return 1
        else:
            return 0
    except:
        return 2


#################################################################################################################################
#               Suspicious TLD
#################################################################################################################################

suspicious_tlds = [
    "fit",
    "tk",
    "gp",
    "ga",
    "work",
    "ml",
    "date",
    "wang",
    "men",
    "icu",
    "online",
    "click",  # Spamhaus
    "country",
    "stream",
    "download",
    "xin",
    "racing",
    "jetzt",
    "ren",
    "mom",
    "party",
    "review",
    "trade",
    "accountants",
    "science",
    "work",
    "ninja",
    "xyz",
    "faith",
    "zip",
    "cricket",
    "win",
    "accountant",
    "realtor",
    "top",
    "christmas",
    "gdn",  # Shady Top-Level Domains
    "link",  # Blue Coat Systems
    "asia",
    "club",
    "la",
    "ae",
    "exposed",
    "pe",
    "go.id",
    "rs",
    "k12.pa.us",
    "or.kr",
    "ce.ke",
    "audio",
    "gob.pe",
    "gov.az",
    "website",
    "bj",
    "mx",
    "media",
    "sa.gov.au",  # statistics
]


def suspicious_tld(tld: str):
    if tld in suspicious_tlds:
        return 1
    return 0


#################################################################################################################################
#               Number of hyperlinks present in a website (Kumar Jain'18)
#################################################################################################################################


def nb_hyperlinks(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
):
    return (
        len(Href["internals"])
        + len(Href["externals"])
        + len(Link["internals"])
        + len(Link["externals"])
        + len(Media["internals"])
        + len(Media["externals"])
        + len(Form["internals"])
        + len(Form["externals"])
        + len(CSS["internals"])
        + len(CSS["externals"])
        + len(Favicon["internals"])
        + len(Favicon["externals"])
    )


#################################################################################################################################
#               Internal hyperlinks ratio (Kumar Jain'18)
#################################################################################################################################


def h_total(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
):
    return nb_hyperlinks(Href, Link, Media, Form, CSS, Favicon)


def h_internal(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
):
    return (
        len(Href["internals"])
        + len(Link["internals"])
        + len(Media["internals"])
        + len(Form["internals"])
        + len(CSS["internals"])
        + len(Favicon["internals"])
    )


def internal_hyperlinks(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
):
    total = h_total(Href, Link, Media, Form, CSS, Favicon)
    if total == 0:
        return 0
    else:
        return h_internal(Href, Link, Media, Form, CSS, Favicon) / total


#################################################################################################################################
#               External hyperlinks ratio (Kumar Jain'18)
#################################################################################################################################


def h_external(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
):
    return (
        len(Href["externals"])
        + len(Link["externals"])
        + len(Media["externals"])
        + len(Form["externals"])
        + len(CSS["externals"])
        + len(Favicon["externals"])
    )


def external_hyperlinks(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
):
    total = h_total(Href, Link, Media, Form, CSS, Favicon)
    if total == 0:
        return 0
    else:
        return h_external(Href, Link, Media, Form, CSS, Favicon) / total


#################################################################################################################################
#               Extrenal CSS (Kumar Jain'18)
#################################################################################################################################


def external_css(CSS: dict[str, Any]):
    return len(CSS["externals"])


#################################################################################################################################
#               Internal redirections (Kumar Jain'18)
#################################################################################################################################


def h_i_error(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
):
    if h_internal(Href, Link, Media, Form, CSS, Favicon) > 10:
        return 0
    count = 0
    for link in Href["internals"]:
        try:
            if requests.get("https://" + link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    for link in Link["internals"]:
        try:
            if requests.get("https://" + link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    for link in Media["internals"]:
        try:
            if requests.get("https://" + link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    for link in Form["internals"]:
        try:
            if requests.get("https://" + link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    for link in CSS["internals"]:
        try:
            if requests.get("https://" + link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    for link in Favicon["internals"]:
        try:
            if requests.get("https://" + link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    return count


def internal_redirection(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
    i_error: int,
):
    internals = h_internal(Href, Link, Media, Form, CSS, Favicon)
    if internals > 0:
        return (internals - i_error) / internals
    return 0


#################################################################################################################################
#               External redirections (Kumar Jain'18)
#################################################################################################################################


def external_redirection(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
    e_error: int,
):
    externals = h_external(Href, Link, Media, Form, CSS, Favicon)
    if externals > 0:
        return (externals - e_error) / externals
    return 0


#################################################################################################################################
#               Generates external errors (Kumar Jain'18)
#################################################################################################################################


def h_e_error(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
):
    if h_external(Href, Link, Media, Form, CSS, Favicon) > 10:
        return 0
    count = 0
    for link in Href["externals"]:
        try:
            if requests.get(link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    for link in Link["externals"]:
        try:
            if requests.get(link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    for link in Media["externals"]:
        try:
            if requests.get(link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    for link in Form["externals"]:
        try:
            if requests.get(link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    for link in CSS["externals"]:
        try:
            if requests.get(link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    for link in Favicon["externals"]:
        try:
            if requests.get(link, timeout=1).status_code >= 400:
                count += 1
        except:
            continue
    return count


def external_errors(
    Href: dict[str, Any],
    Link: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
    e_error: int,
):
    externals = h_external(Href, Link, Media, Form, CSS, Favicon)
    if externals > 0:
        return e_error / externals
    return 0


#################################################################################################################################
#               Having login form link (Kumar Jain'18)
#################################################################################################################################


def login_form(Form: dict[str, Any]):
    p = re.compile(r"([a-zA-Z0-9\_])+.php")
    if len(Form["externals"]) > 0 or len(Form["null"]) > 0:
        return 1
    for form in Form["internals"] + Form["externals"]:
        if p.match(form) is not None:
            return 1
    return 0


#################################################################################################################################
#               Having external favicon (Kumar Jain'18)
#################################################################################################################################


def external_favicon(Favicon: dict[str, Any]):
    if len(Favicon["externals"]) > 0:
        return 1
    return 0


#################################################################################################################################
#               Submitting to email
#################################################################################################################################


def submitting_to_email(Form: dict[str, Any]):
    for form in Form["internals"] + Form["externals"]:
        if "mailto:" in form or "mail()" in form:
            return 1
        else:
            return 0
    return 0


#################################################################################################################################
#               Percentile of internal media <= 61 : Request URL in Zaini'2019
#################################################################################################################################


def internal_media(Media: dict[str, Any]):
    total = len(Media["internals"]) + len(Media["externals"])
    internals = len(Media["internals"])
    try:
        percentile = internals / float(total) * 100
    except:
        return 0

    return percentile


#################################################################################################################################
#               Percentile of external media : Request URL in Zaini'2019
#################################################################################################################################


def external_media(Media: dict[str, Any]):
    total = len(Media["internals"]) + len(Media["externals"])
    externals = len(Media["externals"])
    try:
        percentile = externals / float(total) * 100
    except:
        return 0

    return percentile


#################################################################################################################################
#               Check for empty title
#################################################################################################################################


def empty_title(Title: str | None):
    if Title:
        return 0
    return 1


#################################################################################################################################
#               Percentile of safe anchor : URL_of_Anchor in Zaini'2019 (Kumar Jain'18)
#################################################################################################################################


def safe_anchor(Anchor: dict[str, Any]):
    total = len(Anchor["safe"]) + len(Anchor["unsafe"])
    unsafe = len(Anchor["unsafe"])
    try:
        percentile = unsafe / float(total) * 100
    except:
        return 0
    return percentile


#################################################################################################################################
#               Percentile of internal links : links_in_tags in Zaini'2019 but without <Meta> tag
#################################################################################################################################


def links_in_tags(Link: dict[str, Any]):
    total = len(Link["internals"]) + len(Link["externals"])
    internals = len(Link["internals"])
    try:
        percentile = internals / float(total) * 100
    except:
        return 0
    return percentile


#################################################################################################################################
#              IFrame Redirection
#################################################################################################################################


def iframe(IFrame: dict[str, Any]):
    if len(IFrame["invisible"]) > 0:
        return 1
    return 0


#################################################################################################################################
#              Pop up window
#################################################################################################################################


def popup_window(content: str):
    if "prompt(" in str(content).lower():
        return 1
    else:
        return 0


#################################################################################################################################
#              Domain in page title (Shirazi'18)
#################################################################################################################################


def domain_in_title(domain: str, title: str):
    try:
        if domain.lower() in title.lower():
            return 0
        return 1
    except:
        return 0


#################################################################################################################################
#              Domain after copyright logo (Shirazi'18)
#################################################################################################################################


def domain_with_copyright(domain: str, content: str):
    try:
        m = re.search(
            r"(\N{COPYRIGHT SIGN}|\N{TRADE MARK SIGN}|\N{REGISTERED SIGN})", content
        )
        if not m:
            return 0
        _copyright = content[m.span()[0] - 50 : m.span()[0] + 50]
        if domain.lower() in _copyright.lower():
            return 0
        else:
            return 1
    except:
        return 0


#################################################################################################################################
#               Domain registration age
#################################################################################################################################


def domain_registration_length(host: whois.WhoisEntry | None):
    try:
        if not host:
            return 0
        expiration_date = host.expiration_date
        today = time.strftime("%Y-%m-%d")
        today = datetime.strptime(today, "%Y-%m-%d")
        # Some domains do not have expiration dates. The application should not raise an error if this is the case.
        if expiration_date:
            if type(expiration_date) == list:
                expiration_date = min(expiration_date)
            return abs((expiration_date - today).days)
        else:
            return 0
    except:
        return -1


#################################################################################################################################
#               Domain recognized by WHOIS
#################################################################################################################################


def whois_registered_domain(host: whois.WhoisEntry | None, domain: str):
    try:
        if not host:
            return 0
        hostname = host.domain_name
        if type(hostname) == list:
            for host_ in hostname:
                if re.search(host_.lower(), domain):
                    return 0
            return 1
        else:
            if not hostname or re.search(hostname.lower(), domain):
                return 0
            else:
                return 1
    except:
        return 1


#################################################################################################################################
#               Domain age of a url
#################################################################################################################################


def domain_age(host: whois.WhoisEntry | None):
    try:
        if not host:
            return 0
        creation_date = host.creation_date
        expiration_date = host.expiration_date
        # Convert `creation_date` to a `datetime` if it's a list
        if isinstance(creation_date, list):
            creation_date = creation_date[0] if creation_date else None
        if isinstance(creation_date, str):
            try:
                creation_date = datetime.strptime(creation_date, "%Y-%m-%d")
            except ValueError:
                return 1

        # Convert `expiration_date` to a `datetime` if it's a list
        if isinstance(expiration_date, list):
            expiration_date = expiration_date[0] if expiration_date else None
        if isinstance(expiration_date, str):
            try:
                expiration_date = datetime.strptime(expiration_date, "%Y-%m-%d")
            except ValueError:
                return 1

        # Check for any remaining None values after parsing
        if creation_date is None or expiration_date is None:
            return 1

        # Calculate the domain age in days
        age_of_domain = abs((expiration_date - creation_date).days)

        # Return 1 if the domain age is less than 6 months, otherwise return 0
        return 1 if (age_of_domain / 30) < 6 else 0

    except Exception as e:
        # Log or handle the exception as needed
        print(f"An error occurred: {e}")
        return -1


#################################################################################################################################
#               Google index
#################################################################################################################################


def google_index(url: str):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; rv:91.0) Gecko/20100101 Firefox/91.0"
        }

        google = "https://www.google.com/search?q=site:" + url + "&hl=en"
        response = requests.get(google, headers=headers, timeout=3)
        soup = BeautifulSoup(response.content, "html.parser")
        not_indexed = re.compile("did not match any documents")

        if soup(text=not_indexed):
            return 0
        else:
            return 1
    except:
        return 0


#################################################################################################################################
#               DNSRecord  expiration length
#################################################################################################################################


def dns_record(domain: str):
    try:
        nameservers = dns.resolver.resolve(domain, "NS")
        if len(nameservers) > 0:
            return 0
        else:
            return 1
    except:
        return 1


#################################################################################################################################
#               Page Rank from OPR
#################################################################################################################################


def page_rank(result: dict[str, Any] | None):
    if not result:
        print("warn: got empty result json")
        return 0
    try:
        result = result["response"][0]["page_rank_integer"]
        if result:
            return result
        else:
            return 0
    except:
        return -1


def rank(result: dict[str, Any] | None):
    if not result:
        print("warn: got empty result json")
        return 0
    try:
        result = result["response"][0]["rank"]
        if result:
            return result
        else:
            return 0
    except:
        return -1


def domainEnd(host: whois.WhoisEntry | None):
    try:
        if not host:
            return 0
        expiration_date = host.expiration_date
        if isinstance(expiration_date, str):
            try:
                expiration_date = datetime.strptime(expiration_date, "%Y-%m-%d")
            except:
                return 1
        if expiration_date is None:
            return 1
        elif type(expiration_date) is list:
            return 1
        else:
            today = datetime.now()
            end = abs((expiration_date - today).days)
            if (end / 30) < 6:
                end = 0
            else:
                end = 1
        return end
    except:
        return -1


def is_URL_accessible(url: str):
    page = None
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36",
            "Referer": "https://targetwebsite.com/page1",
        }
        page = requests.get(url, headers=headers, timeout=10)
    except:
        pass

    if page and page.status_code == 200 and page.content not in ["b''", "b' '"]:
        return url, page
    else:
        return None


#################################################################################################################################
#              Data Extraction Process
#################################################################################################################################


def extract_data_from_URL(
    hostname: str,
    content: bytes,
    domain: str,
    Href: dict[str, Any],
    Link: dict[str, Any],
    Anchor: dict[str, Any],
    Media: dict[str, Any],
    Form: dict[str, Any],
    CSS: dict[str, Any],
    Favicon: dict[str, Any],
    IFrame: dict[str, Any],
    Title: str,
    Text: str,
):
    Null_format = [
        "",
        "#",
        "#nothing",
        "#doesnotexist",
        "#null",
        "#void",
        "#whatever",
        "#content",
        "javascript::void(0)",
        "javascript::void(0);",
        "javascript::;",
        "javascript",
    ]

    soup = BeautifulSoup(content, "html.parser", from_encoding="iso-8859-1")

    # collect all external and internal hrefs from url
    for href in soup.find_all("a", href=True):
        dots = [x.start(0) for x in re.finditer(r"\.", href["href"])]
        if (
            hostname in href["href"]
            or domain in href["href"]
            or len(dots) == 1
            or not href["href"].startswith("http")
        ):
            if (
                "#" in href["href"]
                or "javascript" in href["href"].lower()
                or "mailto" in href["href"].lower()
            ):
                Anchor["unsafe"].append(href["href"])
            if not href["href"].startswith("http"):
                if not href["href"].startswith("/"):
                    Href["internals"].append(hostname + "/" + href["href"])
                elif href["href"] in Null_format:
                    Href["null"].append(href["href"])
                else:
                    Href["internals"].append(hostname + href["href"])
        else:
            Href["externals"].append(href["href"])
            Anchor["safe"].append(href["href"])

    # collect all media src tags
    for img in soup.find_all("img", src=True):
        dots = [x.start(0) for x in re.finditer(r"\.", img["src"])]
        if (
            hostname in img["src"]
            or domain in img["src"]
            or len(dots) == 1
            or not img["src"].startswith("http")
        ):
            if not img["src"].startswith("http"):
                if not img["src"].startswith("/"):
                    Media["internals"].append(hostname + "/" + img["src"])
                elif img["src"] in Null_format:
                    Media["null"].append(img["src"])
                else:
                    Media["internals"].append(hostname + img["src"])
        else:
            Media["externals"].append(img["src"])

    for audio in soup.find_all("audio", src=True):
        dots = [x.start(0) for x in re.finditer(r"\.", audio["src"])]
        if (
            hostname in audio["src"]
            or domain in audio["src"]
            or len(dots) == 1
            or not audio["src"].startswith("http")
        ):
            if not audio["src"].startswith("http"):
                if not audio["src"].startswith("/"):
                    Media["internals"].append(hostname + "/" + audio["src"])
                elif audio["src"] in Null_format:
                    Media["null"].append(audio["src"])
                else:
                    Media["internals"].append(hostname + audio["src"])
        else:
            Media["externals"].append(audio["src"])

    for embed in soup.find_all("embed", src=True):
        dots = [x.start(0) for x in re.finditer(r"\.", embed["src"])]
        if (
            hostname in embed["src"]
            or domain in embed["src"]
            or len(dots) == 1
            or not embed["src"].startswith("http")
        ):
            if not embed["src"].startswith("http"):
                if not embed["src"].startswith("/"):
                    Media["internals"].append(hostname + "/" + embed["src"])
                elif embed["src"] in Null_format:
                    Media["null"].append(embed["src"])
                else:
                    Media["internals"].append(hostname + embed["src"])
        else:
            Media["externals"].append(embed["src"])

    for i_frame in soup.find_all("iframe", src=True):
        dots = [x.start(0) for x in re.finditer(r"\.", i_frame["src"])]
        if (
            hostname in i_frame["src"]
            or domain in i_frame["src"]
            or len(dots) == 1
            or not i_frame["src"].startswith("http")
        ):
            if not i_frame["src"].startswith("http"):
                if not i_frame["src"].startswith("/"):
                    Media["internals"].append(hostname + "/" + i_frame["src"])
                elif i_frame["src"] in Null_format:
                    Media["null"].append(i_frame["src"])
                else:
                    Media["internals"].append(hostname + i_frame["src"])
        else:
            Media["externals"].append(i_frame["src"])

    # collect all link tags
    for link in soup.findAll("link", href=True):
        dots = [x.start(0) for x in re.finditer(r"\.", link["href"])]
        if (
            hostname in link["href"]
            or domain in link["href"]
            or len(dots) == 1
            or not link["href"].startswith("http")
        ):
            if not link["href"].startswith("http"):
                if not link["href"].startswith("/"):
                    Link["internals"].append(hostname + "/" + link["href"])
                elif link["href"] in Null_format:
                    Link["null"].append(link["href"])
                else:
                    Link["internals"].append(hostname + link["href"])
        else:
            Link["externals"].append(link["href"])

    for script in soup.find_all("script", src=True):
        dots = [x.start(0) for x in re.finditer(r"\.", script["src"])]
        if (
            hostname in script["src"]
            or domain in script["src"]
            or len(dots) == 1
            or not script["src"].startswith("http")
        ):
            if not script["src"].startswith("http"):
                if not script["src"].startswith("/"):
                    Link["internals"].append(hostname + "/" + script["src"])
                elif script["src"] in Null_format:
                    Link["null"].append(script["src"])
                else:
                    Link["internals"].append(hostname + script["src"])
        else:
            try:
                Link["externals"].append(script["href"])
            except:
                pass

    # collect all css
    for link in soup.find_all("link", rel="stylesheet"):
        if "href" in link.attrs:
            dots = [x.start(0) for x in re.finditer(r"\.", link["href"])]
            if (
                hostname in link["href"]
                or domain in link["href"]
                or len(dots) == 1
                or not link["href"].startswith("http")
            ):
                if not link["href"].startswith("http"):
                    if not link["href"].startswith("/"):
                        CSS["internals"].append(hostname + "/" + link["href"])
                    elif link["href"] in Null_format:
                        CSS["null"].append(link["href"])
                    else:
                        CSS["internals"].append(hostname + link["href"])
            else:
                CSS["externals"].append(link["href"])

    for style in soup.find_all("style", type="text/css"):
        try:
            start = str(style[0]).index("@import url(")
            end = str(style[0]).index(")")
            css = str(style[0])[start + 12 : end]
            dots = [x.start(0) for x in re.finditer(r"\.", css)]
            if (
                hostname in css
                or domain in css
                or len(dots) == 1
                or not css.startswith("http")
            ):
                if not css.startswith("http"):
                    if not css.startswith("/"):
                        CSS["internals"].append(hostname + "/" + css)
                    elif css in Null_format:
                        CSS["null"].append(css)
                    else:
                        CSS["internals"].append(hostname + css)
            else:
                CSS["externals"].append(css)
        except:
            continue

    # collect all form actions
    for form in soup.findAll("form", action=True):
        dots = [x.start(0) for x in re.finditer(r"\.", form["action"])]
        if (
            hostname in form["action"]
            or domain in form["action"]
            or len(dots) == 1
            or not form["action"].startswith("http")
        ):
            if not form["action"].startswith("http"):
                if not form["action"].startswith("/"):
                    Form["internals"].append(hostname + "/" + form["action"])
                elif form["action"] in Null_format or form["action"] == "about:blank":
                    Form["null"].append(form["action"])
                else:
                    Form["internals"].append(hostname + form["action"])
        else:
            Form["externals"].append(form["action"])

    # collect all link tags
    for head in soup.find_all("head"):
        for head.link in soup.find_all("link", href=True):
            dots = [x.start(0) for x in re.finditer(r"\.", head.link["href"])]
            if (
                hostname in head.link["href"]
                or len(dots) == 1
                or domain in head.link["href"]
                or not head.link["href"].startswith("http")
            ):
                if not head.link["href"].startswith("http"):
                    if not head.link["href"].startswith("/"):
                        Favicon["internals"].append(hostname + "/" + head.link["href"])
                    elif head.link["href"] in Null_format:
                        Favicon["null"].append(head.link["href"])
                    else:
                        Favicon["internals"].append(hostname + head.link["href"])
            else:
                Favicon["externals"].append(head.link["href"])

        for head.link in soup.findAll("link", {"href": True, "rel": True}):
            isicon = False
            if isinstance(head.link["rel"], list):
                for e_rel in head.link["rel"]:
                    if e_rel.endswith("icon"):
                        isicon = True
            else:
                if head.link["rel"].endswith("icon"):
                    isicon = True

            if isicon:
                dots = [x.start(0) for x in re.finditer(r"\.", head.link["href"])]
                if (
                    hostname in head.link["href"]
                    or len(dots) == 1
                    or domain in head.link["href"]
                    or not head.link["href"].startswith("http")
                ):
                    if not head.link["href"].startswith("http"):
                        if not head.link["href"].startswith("/"):
                            Favicon["internals"].append(
                                hostname + "/" + head.link["href"]
                            )
                        elif head.link["href"] in Null_format:
                            Favicon["null"].append(head.link["href"])
                        else:
                            Favicon["internals"].append(hostname + head.link["href"])
                else:
                    Favicon["externals"].append(head.link["href"])

    # collect i_frame
    for i_frame in soup.find_all("iframe", width=True, height=True, frameborder=True):
        if (
            i_frame["width"] == "0"
            and i_frame["height"] == "0"
            and i_frame["frameborder"] == "0"
        ):
            IFrame["invisible"].append(i_frame)
        else:
            IFrame["visible"].append(i_frame)
    for i_frame in soup.find_all("iframe", width=True, height=True, border=True):
        if (
            i_frame["width"] == "0"
            and i_frame["height"] == "0"
            and i_frame["border"] == "0"
        ):
            IFrame["invisible"].append(i_frame)
        else:
            IFrame["visible"].append(i_frame)
    for i_frame in soup.find_all("iframe", width=True, height=True, style=True):
        if (
            i_frame["width"] == "0"
            and i_frame["height"] == "0"
            and i_frame["style"] == "border:none;"
        ):
            IFrame["invisible"].append(i_frame)
        else:
            IFrame["visible"].append(i_frame)

    # get page title
    try:
        if soup.title and soup.title.string:
            Title = soup.title.string
    except:
        pass

    # get content text
    Text = soup.get_text()

    return Href, Link, Anchor, Media, Form, CSS, Favicon, IFrame, Title, Text


def extract_features(urls: list[str], url: str):
    def get_domain(url: str):
        o = urllib.parse.urlsplit(url)
        return o.netloc, tldextract.extract(url).domain, o.path

    def words_raw_extraction(domain: str, subdomain: str, path: str):
        w_domain = re.split(r"\-|\.|\/|\?|\=|\@|\&|\%|\:|\_", domain.lower())
        w_subdomain = re.split(r"\-|\.|\/|\?|\=|\@|\&|\%|\:|\_", subdomain.lower())
        w_path = re.split(r"\-|\.|\/|\?|\=|\@|\&|\%|\:|\_", path.lower())
        raw_words = w_domain + w_path + w_subdomain
        w_host = w_domain + w_subdomain
        raw_words = list(filter(None, raw_words))
        return raw_words, list(filter(None, w_host)), list(filter(None, w_path))

    Href = {"internals": [], "externals": [], "null": []}
    Link = {"internals": [], "externals": [], "null": []}
    Anchor = {"safe": [], "unsafe": [], "null": []}
    Media = {"internals": [], "externals": [], "null": []}
    Form = {"internals": [], "externals": [], "null": []}
    CSS = {"internals": [], "externals": [], "null": []}
    Favicon = {"internals": [], "externals": [], "null": []}
    IFrame = {"visible": [], "invisible": [], "null": []}
    Title = ""
    Text = ""

    # Kiểm tra khả năng truy cập URL
    accessible_url = is_URL_accessible(url)
    if accessible_url:
        iurl, page = accessible_url
        content = page.content
        hostname, domain, path = get_domain(url)
        extracted_domain = tldextract.extract(url)
        domain = extracted_domain.domain + "." + extracted_domain.suffix
        subdomain = extracted_domain.subdomain
        tmp = url[url.find(extracted_domain.suffix) : len(url)]
        pth = tmp.partition("/")
        path = pth[1] + pth[2]
        words_raw, words_raw_host, words_raw_path = words_raw_extraction(
            extracted_domain.domain, subdomain, pth[2]
        )
        tld = extracted_domain.suffix
        parsed = urlparse(url)
        scheme = parsed.scheme

        try:
            host = whois.whois(domain)
        except:
            host = None

        try:
            key = "c4skc4o8kswocso0og84w4gk44so048k8og44000"
            rank_domain = (
                "https://openpagerank.com/api/v1.0/getPageRank?domains%5B0%5D=" + domain
            )
            request = requests.get(rank_domain, headers={"API-OPR": key}, timeout=5)
            result_json = request.json()
        except:
            result_json = None

        (
            Href,
            Link,
            Anchor,
            Media,
            Form,
            CSS,
            Favicon,
            IFrame,
            Title,
            Text,
        ) = extract_data_from_URL(
            hostname,
            content,
            domain,
            Href,
            Link,
            Anchor,
            Media,
            Form,
            CSS,
            Favicon,
            IFrame,
            Title,
            Text,
        )
        e_error = h_e_error(Href, Link, Media, Form, CSS, Favicon)
        i_error = h_i_error(Href, Link, Media, Form, CSS, Favicon)

        # Trả về một dictionary hợp lệ
        row = {
            "url": url,
            "length_url": url_length(url),
            "length_hostname": url_length(hostname),
            "ip": having_ip_address(url),
            "nb_dots": count_dots(url),
            "nb_hyphens": count_hyphens(url),
            "nb_at": count_at(url),
            "nb_qm": count_exclamation(url),
            "nb_and": count_and(url),
            "nb_or": count_or(url),
            "nb_eq": count_equal(url),
            "nb_underscore": count_underscore(url),
            "nb_tilde": count_tilde(url),
            "nb_percent": count_percentage(url),
            "nb_slash": count_slash(url),
            "nb_star": count_star(url),
            "nb_colon": count_colon(url),
            "nb_comma": count_comma(url),
            "nb_semicolon": count_semicolon(url),
            "nb_dollar": count_dollar(url),
            "nb_space": count_space(url),
            "nb_www": check_www(words_raw),
            "nb_com": check_com(words_raw),
            "nb_dslash": count_double_slash(url),
            "http_in_path": count_http_token(path),
            "https_token": https_token(scheme),
            "ratio_digits_url": count_digits(url),
            "ratio_digits_host": ratio_digits(hostname),
            "punycode": punycode(url),
            "port": port(url),
            "tld_in_path": tld_in_path(tld, path),
            "tld_in_subdomain": tld_in_subdomain(tld, subdomain),
            "abnormal_subdomain": abnormal_subdomain(url),
            "nb_subdomains": count_subdomain(url),
            "prefix_suffix": prefix_suffix(url),
            "shortening_service": shortening_service(url),
            "path_extension": path_extension(path),
            "nb_redirection": count_redirection(page),
            "nb_external_redirection": count_external_redirection(page, domain),
            "length_words_raw": length_word_raw(words_raw),
            "char_repeat": char_repeat(words_raw),
            "shortest_words_raw": shortest_word_length(words_raw),
            "shortest_word_host": shortest_word_length(words_raw_host),
            "shortest_word_path": shortest_word_length(words_raw_path),
            "longest_words_raw": longest_word_length(words_raw),
            "longest_word_host": longest_word_length(words_raw_host),
            "longest_word_path": longest_word_length(words_raw_path),
            "avg_words_raw": average_word_length(words_raw),
            "avg_word_host": average_word_length(words_raw_host),
            "avg_word_path": average_word_length(words_raw_path),
            "phish_hints": phish_hints(url),
            "domain_in_brand": domain_in_brand(extracted_domain.domain, urls),
            "brand_in_subdomain": brand_in_path(
                extracted_domain.domain, subdomain, urls
            ),
            "brand_in_path": brand_in_path(extracted_domain.domain, path, urls),
            "suspecious_tld": suspicious_tld(tld),
            "statistical_report": statistical_report(url, domain),
            "nb_hyperlinks": nb_hyperlinks(Href, Link, Media, Form, CSS, Favicon),
            "ratio_intHyperlinks": internal_hyperlinks(
                Href, Link, Media, Form, CSS, Favicon
            ),
            "ratio_extHyperlinks": external_hyperlinks(
                Href, Link, Media, Form, CSS, Favicon
            ),
            "nb_extCSS": external_css(CSS),
            "ratio_intRedirection": internal_redirection(
                Href, Link, Media, Form, CSS, Favicon, i_error
            ),
            "ratio_extRedirection": external_redirection(
                Href, Link, Media, Form, CSS, Favicon, e_error
            ),
            "ratio_extErrors": external_errors(
                Href, Link, Media, Form, CSS, Favicon, e_error
            ),
            "login_form": login_form(Form),
            "external_favicon": external_favicon(Favicon),
            "links_in_tags": links_in_tags(Link),
            "submit_email": submitting_to_email(Form),
            "ratio_intMedia": internal_media(Media),
            "ratio_extMedia": external_media(Media),
            "iframe": iframe(IFrame),
            "popup_window": popup_window(Text),
            "safe_anchor": safe_anchor(Anchor),
            "empty_title": empty_title(Title),
            "domain_in_title": domain_in_title(extracted_domain.domain, Title),
            "domain_with_copyright": domain_with_copyright(
                extracted_domain.domain, Text
            ),
            "whois_registered_domain": whois_registered_domain(host, domain),
            "domain_registration_length": domain_registration_length(host),
            "domain_age": domain_age(host),
            "dns_record": dns_record(domain),
            "google_index": google_index(url),
            "page_rank": page_rank(result_json),
            "rank": rank(result_json),
            "domainEnd": domainEnd(host),
        }
        return row
    else:
        # Trả về một dictionary thay vì tuple khi URL không hợp lệ
        return {
            "url": url,
            "length_url": None,
            "length_hostname": None,
            "ip": None,
            "nb_dots": None,
            "nb_hyphens": None,
            "nb_at": None,
            "nb_qm": None,
            "nb_and": None,
            "nb_or": None,
            "nb_eq": None,
            "nb_underscore": None,
            "nb_tilde": None,
            "nb_percent": None,
            "nb_slash": None,
            "nb_star": None,
            "nb_colon": None,
            "nb_comma": None,
            "nb_semicolon": None,
            "nb_dollar": None,
            "nb_space": None,
            "nb_www": None,
            "nb_com": None,
            "nb_dslash": None,
            "http_in_path": None,
            "https_token": None,
            "ratio_digits_url": None,
            "ratio_digits_host": None,
            "punycode": None,
            "port": None,
            "tld_in_path": None,
            "tld_in_subdomain": None,
            "abnormal_subdomain": None,
            "nb_subdomains": None,
            "prefix_suffix": None,
            "shortening_service": None,
            "path_extension": None,
            "nb_redirection": None,
            "nb_external_redirection": None,
            "length_words_raw": None,
            "char_repeat": None,
            "shortest_words_raw": None,
            "shortest_word_host": None,
            "shortest_word_path": None,
            "longest_words_raw": None,
            "longest_word_host": None,
            "longest_word_path": None,
            "avg_words_raw": None,
            "avg_word_host": None,
            "avg_word_path": None,
            "phish_hints": None,
            "domain_in_brand": None,
            "brand_in_subdomain": None,
            "brand_in_path": None,
            "suspecious_tld": None,
            "statistical_report": None,
            "nb_hyperlinks": None,
            "ratio_intHyperlinks": None,
            "ratio_extHyperlinks": None,
            "nb_extCSS": None,
            "ratio_intRedirection": None,
            "ratio_extRedirection": None,
            "ratio_extErrors": None,
            "login_form": None,
            "external_favicon": None,
            "links_in_tags": None,
            "submit_email": None,
            "ratio_intMedia": None,
            "ratio_extMedia": None,
            "iframe": None,
            "popup_window": None,
            "safe_anchor": None,
            "empty_title": None,
            "domain_in_title": None,
            "domain_with_copyright": None,
            "whois_registered_domain": None,
            "domain_registration_length": None,
            "domain_age": None,
            "dns_record": None,
            "google_index": None,
            "page_rank": None,
            "rank": None,
            "domainEnd": None,
        }


def addingCSV(file_name, status_value):
    # Đọc URL từ file (active_legitimate_urls.txt hoặc active_phishing_urls.txt)
    with open(file_name, "r", encoding="utf-8") as f:
        urls = [line.rstrip() for line in f.readlines()]

    # Hàm xử lý song song với batch 100 URL
    def process_batch(batch_urls):
        return [extract_features(batch_urls, url) for url in batch_urls]

    # Tạo các batch 100 URL
    batch_size = 100
    url_batches = [urls[i : i + batch_size] for i in range(0, len(urls), batch_size)]

    # Sử dụng ThreadPoolExecutor để xử lý song song
    with ThreadPoolExecutor(max_workers=100) as executor:
        results = list(executor.map(process_batch, url_batches))

    # Flatten danh sách kết quả
    mapped_data = [item for batch in results for item in batch]

    # Kiểm tra và lọc dữ liệu hợp lệ (dạng dictionary)
    valid_data = [item for item in mapped_data if isinstance(item, dict)]

    # Báo lỗi nếu không có dữ liệu hợp lệ
    if not valid_data:
        print(f"Không có dữ liệu hợp lệ trong {file_name}.")
        return pd.DataFrame()  # Trả về DataFrame trống nếu không có dữ liệu hợp lệ

    # Tạo DataFrame từ dữ liệu đã lọc
    df_new = pd.DataFrame(valid_data)

    # Gán giá trị status theo nguồn tệp
    df_new["status"] = status_value

    return df_new


def process_and_save():
    # Nhập dữ liệu từ cả hai tệp và gán giá trị status
    df_legitimate = addingCSV("active_legitimate_urls.txt", "legitimate")
    df_phishing = addingCSV("new_phishing_urls.txt", "phishing")

    # Kiểm tra nếu cả hai DataFrame đều trống
    if df_legitimate.empty and df_phishing.empty:
        print("Không có dữ liệu hợp lệ từ cả hai tệp.")
        return

    # Chỉ hợp nhất dữ liệu nếu ít nhất một DataFrame có dữ liệu
    if not df_legitimate.empty and not df_phishing.empty:
        df_combined = pd.concat(
            [df_legitimate, df_phishing], ignore_index=True
        ).drop_duplicates(subset="url", keep="last")
    elif not df_legitimate.empty:  # Nếu chỉ có dữ liệu từ tệp hợp lệ của legitimate
        df_combined = df_legitimate
    else:  # Nếu chỉ có dữ liệu từ tệp hợp lệ của phishing
        df_combined = df_phishing

    # Kiểm tra xem file CSV đã tồn tại chưa
    try:
        df_existing = pd.read_csv("output.csv", encoding="utf-8")
        # Hợp nhất dữ liệu mới và cũ, tránh trùng lặp URL
        df_combined = pd.concat(
            [df_existing, df_combined], ignore_index=True
        ).drop_duplicates(subset="url", keep="last")
    except FileNotFoundError:
        # Nếu chưa có file cũ, chỉ lưu dữ liệu mới
        pass

    # Ghi dữ liệu vào file CSV
    df_combined.to_csv("output.csv", index=False, encoding="utf-8")
    print("Dữ liệu đã được cập nhật và lưu vào file csv từ cả hai tệp.")


def dataset_filtered():
    # Đọc file CSV
    df = pd.read_csv("output.csv")

    # Loại bỏ cột cuối cùng
    df = df.iloc[:, :-1]  # Chọn tất cả các cột trừ cột cuối cùng

    # Tính số lượng giá trị rỗng trên mỗi hàng (NaN hoặc chuỗi rỗng)
    empty_counts = df.isna().sum(axis=1) + (df == "").sum(axis=1)

    # Lọc các hàng có trên 2 giá trị rỗng
    df_filtered = df[empty_counts <= 2].copy()  # Giữ các hàng có tối đa 2 giá trị rỗng

    # Thay thế giá trị rỗng bằng 0
    df_filtered.fillna(0, inplace=True)  # Thay NaN bằng 0
    df_filtered.replace("", 0, inplace=True)  # Thay chuỗi rỗng bằng 0

    # Lưu kết quả vào một file CSV mới
    df_filtered.to_csv("filter_output.csv", index=False)
    print("Đã lọc và xử lý thành công!")
    print(f"Số hàng ban đầu: {len(df)}, Số hàng sau khi lọc: {len(df_filtered)}")


def main():
    process_and_save()
    dataset_filtered()


if __name__ == "__main__":
    main()
